{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b17f989-cec1-4d13-8256-4dddf3338e75",
   "metadata": {},
   "source": [
    "# Código para identificar tópicos presentes en el corpus noticioso de 'El Siglo de Torreón'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7901c326-fb9d-44b2-bb7a-6bda3f84e5cd",
   "metadata": {},
   "source": [
    "## Importes Necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8a1fc7-75e4-47e5-8524-3786c7706aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numpy and Pandas (necesarios para Spacy)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "#gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "#NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "#spacy y stanza\n",
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "#mysql\n",
    "import mysql.connector as sql\n",
    "\n",
    "#extras\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0779f0-b90c-4554-b4d5-9b30ab605061",
   "metadata": {},
   "source": [
    "## Definiciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ace3be-25a3-410e-b22d-a9ba7a685a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definición de constantes para conexión MySQL\n",
    "host_c=\"localhost\"\n",
    "port_c=3306\n",
    "user_c=\"root\"\n",
    "passwd_c=\"Password1234!\"\n",
    "db_c=\"siglodb_2002-2022\"\n",
    "\n",
    "conn = sql.connect(host=host_c, port=port_c, user=user_c, passwd=passwd_c, db=db_c)\n",
    "cursor = conn.cursor(buffered=True)      \n",
    "\n",
    "exceptions = {\"biden\", \"otan\", \"florida\"}\n",
    "\n",
    "#Función para preprocesamiento de textos utilizando Gensim\n",
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True, min_len=4)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "#Función para remover palabras de uso común\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "#Funciones para creación de bigramas y trigramas\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "#Función para lematización de los textos\n",
    "def lemmatization(texts):\n",
    "    texts_out = []\n",
    "    for text in tqdm(texts):\n",
    "        doc = nlp(\" \".join(text)) \n",
    "        lemmatized_text = []\n",
    "        for token in doc:\n",
    "            if token.text.lower() in exceptions:\n",
    "                lemmatized_text.append(token.text)  # Keep the original word\n",
    "            else:\n",
    "                lemmatized_text.append(token.lemma_)\n",
    "        texts_out.append(lemmatized_text)\n",
    "    return texts_out\n",
    "\n",
    "#Función para evaluar puntaje de coherencia de los tópicos resultantes\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=4):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in tqdm(range(start, limit, step)):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics, alpha='auto', eta=.5, passes=20, chunksize=100, update_every=1, random_state=100)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        \n",
    "    return model_list, coherence_values\n",
    "\n",
    "#Función para ordenar los tópicos\n",
    "def Sort(sub_li):\n",
    "     sub_li.sort(key = lambda x: x[1])\n",
    "     sub_li.reverse()\n",
    "     return (sub_li)\n",
    "\n",
    "#Función para insertar los tópicos resultantes (Clave de tópico, identificador, palabras que componen el tópico, peso de cada palabra dentro del tópico y distribución marginal de cada palabra en el tópico) en la base de datos MySQL\n",
    "def insert_into_mysql(row):\n",
    "    cursor = conn.cursor(buffered=True)\n",
    "    query = \"INSERT INTO siglodb_topicos (topico_clave, topic_num, palabra, peso, distribucion_marginal) VALUES (%s, %s, %s, %s, %s)\"\n",
    "    values = (row['topico_clave'], row['topic_num'], row['palabra'], row['peso'], row['distribucion_marginal'])\n",
    "    cursor.execute(query, values)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "\n",
    "#Función para registrar el tópico de cada una de las noticias en la base de datos general\n",
    "def update_row_in_database(row):\n",
    "    cursor = conn.cursor(buffered=True)\n",
    "    cursor.execute(\"UPDATE siglodb2022 SET topico1 = %s, topico1_peso = %s, \\\n",
    "                    topico2 = %s, topico2_peso = %s \\\n",
    "                    WHERE id = %s\",\n",
    "                   (row['topico1'], row['topico1_peso'], row['topico2'], row['topico2_peso'], row['id']))\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "\n",
    "#Función para extraer tuplas y poder registrar los tópicos de la función anterior\n",
    "def extract_tuple(row):\n",
    "    if row is None:\n",
    "        return pd.Series([None, None])\n",
    "    return pd.Series(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28570df5-abcc-4ed0-8474-a6db17621c9e",
   "metadata": {},
   "source": [
    "## Inicio del análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d1aba2-8238-4957-b9d3-583452db1f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definición de la fecha de inicio de la iteración y la sección a analizar\n",
    "Mes = 1\n",
    "Año = 2022\n",
    "Seccion = \"Regionales\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2d30a-6bcb-41eb-879d-edf9413a038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(13): \n",
    "    if Mes == 13:\n",
    "        break  \n",
    "    \n",
    "    #Conectar a la base de datos en MySQL local\n",
    "    distintivo = f\"{Año}_{Mes}_{Seccion}\"\n",
    "    conn = sql.connect(host=host_c, port=port_c, user=user_c, passwd=passwd_c, db=db_c)\n",
    "    cursor = conn.cursor(buffered=True)      \n",
    "    query = \"SELECT primer_parrafo FROM siglodb2022 WHERE MONTH(fecha) = %s AND YEAR(fecha) = %s AND seccion = %s;\"\n",
    "    cursor.execute(query, (Mes, Año, Seccion))\n",
    "    data = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    titulares = [row[0] for row in data]\n",
    "    \n",
    "    #Tokeniza\n",
    "    data_words = gen_words(titulares)\n",
    "    \n",
    "    #Remueve palabras de uso común\n",
    "    stop_words = stopwords.words('spanish')\n",
    "    stop_words.extend(['cada', 'persona', 'mil', 'tras', 'ademas', 'pues', 'habia', 'debido', 'aunque', 'también', 'tambien', 'lunes', 'martes', 'miércoles', 'jueves', 'sábado', 'domingo', 'miercoles', 'dijo', 'según', 'segun', 'estan', 'están', 'puede', 'debe', 'años', 'año', 'anos', 'ano', 'enero', 'febrero', 'marzo', 'abril', 'mayo', 'junio', 'julio', 'agosto', 'septiembre', 'agosto', 'septiembre', 'octubre', 'noviembre', 'diciembre'])\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "    \n",
    "    #Genera bigramas y trigramas\n",
    "    bigram = gensim.models.Phrases(data_words_nostops, min_count=20, threshold=200)\n",
    "    trigram = gensim.models.Phrases(bigram[data_words_nostops], min_count=20, threshold=200)  \n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "    data_words_trigrams = make_trigrams(data_words_bigrams)\n",
    "    \n",
    "    #Lematiza\n",
    "    data_lemmatized = lemmatization(data_words_trigrams)\n",
    "    \n",
    "    #Genera lista y remueve palabras con alta frecuencia\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "    texts = data_lemmatized\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "    low_value = 0.03\n",
    "    words =[]\n",
    "    words_missing_in_tfidf =[]\n",
    "    for i in range(0, len(corpus)):\n",
    "        bow = corpus[i]\n",
    "        low_value_words = []\n",
    "        tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "        bow_ids = [id for id, value in bow]\n",
    "        low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "        drops = low_value_words+words_missing_in_tfidf\n",
    "        for item in drops:\n",
    "            words.append(id2word[item])\n",
    "        words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids]\n",
    "        new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "        corpus[i] = new_bow\n",
    "    \n",
    "    #Itera por diferentes modelos según número de tópicos y genera puntajes de coherencia\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=texts, start=2, limit=60, step=4)\n",
    "\n",
    "    #Genera una gráfica con los puntajes de coherencia y número de tópicos. Guarda la gráfica a disco duro. \n",
    "    limit=60\n",
    "    start=2\n",
    "    step=4\n",
    "\n",
    "    x = range(start, limit, step)\n",
    "\n",
    "    plt.plot(x, coherence_values, label =f\"{Mes}\")\n",
    "    plt.xlabel(\"Num Tópicos\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.title(f\"{Seccion} - {Año}\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(f\"results/figures/{distintivo}.pdf\")\n",
    "    \n",
    "\n",
    "    best_model_index = coherence_values.index(max(coherence_values))\n",
    "    max_score = max(coherence_values)\n",
    "    best_model = model_list[best_model_index]\n",
    "    best_num_topics = best_model.num_topics\n",
    "\n",
    "    # Obtiene los tópicos del modelo con mejor puntaje de coherencia\n",
    "    topics = best_model.show_topics(formatted=False, num_words=30)\n",
    "\n",
    "     # Itera por cada uno de los tópicos del mejor modelo y genera la distribución marginal. Si el tópico seleccionado está presente en más del 5% de las noticias, lo selecciona para análisis. \n",
    "    selected_topics = []\n",
    "    for topic_id, topic in topics:     \n",
    "        marginal_distribution = sum([prob for _, prob in topic])\n",
    "        if marginal_distribution > 0.005:\n",
    "            selected_topics.append((topic_id, topic, marginal_distribution))\n",
    "\n",
    "    # Genera el número de tópicos seleccionados (aquellos con una distribución marginal mayor a 5%)\n",
    "    num_selected_topics = len(selected_topics)\n",
    "\n",
    "    #Selecciona el mejor modelo y registra los valores en siglodb_topicos_registro, donde se guarda un registro de cada uno de los modelos, número de tópicos resultantes y número de tópicos seleccionados\n",
    "    query = \"INSERT INTO siglodb_topicos_registro (modelo_clave, coherence_score, num_topicos, num_topicos_seleccionados) VALUES (%s, %s, %s, %s);\"\n",
    "    conn = sql.connect(host=host_c, port=port_c, user=user_c, passwd=passwd_c, db=db_c)\n",
    "    cursor = conn.cursor(buffered=True)      \n",
    "    cursor.execute(query, (distintivo, max_score, best_num_topics, num_selected_topics))\n",
    "    cursor.close()\n",
    "    conn.commit()\n",
    "    \n",
    "    # #Genera la lista de palabras por tópico\n",
    "    lda_data = []\n",
    "    for topic_num, words, marginal_distribution in selected_topics:\n",
    "        for word, weight in words:\n",
    "            lda_data.append([f\"{distintivo}\", topic_num, word, weight, marginal_distribution])\n",
    "\n",
    "    topics_df = pd.DataFrame(lda_data, columns=['topico_clave', 'topic_num', 'palabra', 'peso', 'distribucion_marginal'])\n",
    "    topics_df['topic_num'] = topics_df['topic_num'].astype(str)\n",
    "    topics_df['topico_clave'] = topics_df['topico_clave'].str.cat(topics_df['topic_num'], sep='_')\n",
    "    \n",
    "    conn = sql.connect(host=host_c, port=port_c, user=user_c, passwd=passwd_c, db=db_c)\n",
    "    topics_df.apply(insert_into_mysql, axis=1)\n",
    "    conn.close()\n",
    "    \n",
    "    #Guarda el modelo en disco duro\n",
    "    foldername = f\"results/models/{distintivo}\"\n",
    "    os.makedirs(foldername, exist_ok=True)\n",
    "    best_model.save(f\"{foldername}/{distintivo}.model\")\n",
    "\n",
    "    #Genera una visualización de los tópicos del modelo seleccionado y la guarda a disco duro\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim.prepare(best_model, corpus, id2word, mds=\"mmds\", R=30)\n",
    "    pyLDAvis.save_html(vis, f\"results/visuals/{distintivo}.html\")\n",
    "\n",
    "    #Ordena la lista de tópicos y palabras\n",
    "    topicos_ordenados = []\n",
    "    \n",
    "    for doc in corpus:\n",
    "        topics = best_model[doc]\n",
    "        topics_sorted = Sort(topics)\n",
    "        topicos_ordenados.append(topics_sorted)\n",
    "    \n",
    "    # Trunca a dos tópicos por noticia\n",
    "    topicos_truncos = []\n",
    "    \n",
    "    for inner_list in topicos_ordenados:\n",
    "        truncated_inner_list = inner_list[:2]\n",
    "        topicos_truncos.append(truncated_inner_list)\n",
    "\n",
    "    #Genera un DataFrame con todos los tópicos\n",
    "    columns = ['Tópico 1', 'Tópico2']\n",
    "    df= pd.DataFrame(topicos_truncos, columns=columns)\n",
    "    num_rows = len(df)\n",
    "    print(num_rows)\n",
    "    \n",
    "    df['topico_clave'] = [distintivo] * num_rows\n",
    "    df[['topico1', 'topico1_peso']] = df['Tópico 1'].apply(extract_tuple)\n",
    "    df[['topico2', 'topico2_peso']] = df['Tópico 2'].apply(extract_tuple)\n",
    "    df.drop(['Tópico 1'], axis=1, inplace=True)\n",
    "    \n",
    "    df['topico1'] = df['topico1'].fillna(-1000)\n",
    "    df['topico1'] = df['topico1'].astype(int)\n",
    "    df['topico2'] = df['topico2'].fillna(-1000)\n",
    "    df['topico2'] = df['topico2'].astype(int)\n",
    "    \n",
    "    df['topico1'] = df['topico1'].astype(str)\n",
    "    df['topico1'] = df['topico_clave'].str.cat(df['topico1'], sep='_')\n",
    "    df['topico2'] = df['topico2'].astype(str)\n",
    "    df['topico2'] = df['topico_clave'].str.cat(df['topico2'], sep='_')\n",
    "    df.drop(['topico_clave'], axis=1, inplace=True)\n",
    "    \n",
    "    null_value = f\"{distintivo}_-1000\"\n",
    "\n",
    "    a_revision = ['topico1', 'topico2']\n",
    "    for col in a_revision:\n",
    "        df.loc[df[col].str.contains(null_value, case=False), col] = None\n",
    "\n",
    "    #Inserta nuevamente los id's de las noticias en el dataframe y actualiza la información mediante MySQL en la base de datos siglodb2022\n",
    "    query = \"SELECT id FROM siglodb2022 WHERE MONTH(fecha) = %s AND YEAR(fecha) = %s AND seccion = %s;\"\n",
    "    conn = sql.connect(host=host_c, port=port_c, user=user_c, passwd=passwd_c, db=db_c)\n",
    "    param_values = (Mes, Año, Seccion)\n",
    "    df_from_sql = pd.read_sql(query, conn, params=param_values)\n",
    "    df['id'] = df_from_sql['id']\n",
    "\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        cursor = conn.cursor(buffered=True)\n",
    "        cursor.execute(\"SELECT * FROM siglodb2022 WHERE id = %s\", (row['id'],))\n",
    "        result = cursor.fetchone()\n",
    "        cursor.close()\n",
    "      \n",
    "        if result:\n",
    "            row.replace({pd.NA: None, np.nan: None}, inplace=True)\n",
    "            update_row_in_database(row)\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "    #Incrementa la variable \"Mes\" para iterar por los diferentes meses del año\n",
    "    Mes += 1 \n",
    "\n",
    "print(\"Año completo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
